{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caac28",
   "metadata": {},
   "source": [
    "### Loading dataset and adapating dataset for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b264108813884c97b67740d913422967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 259k/259k [00:00<00:00, 2.68MB/s]\n",
      "Downloading data: 100%|██████████| 34.3k/34.3k [00:00<00:00, 452kB/s]\n",
      "Downloading data: 100%|██████████| 76.1k/76.1k [00:00<00:00, 982kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c0fec31f8d44c9b26b71f9ccdce9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0957a573a44589af38ec4f8622efc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9109848314764ec5b4e84c4bcaf4ed8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index_id', 'category', 'text'],\n",
       "        num_rows: 701\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index_id', 'category', 'text'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index_id', 'category', 'text'],\n",
       "        num_rows: 204\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the telugu language dataset\n",
    "ds = load_dataset('Davlan/sib200', 'tel_Telu')\n",
    "# Show ds\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a5467d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36d5991b8ef45a099729dd96879d621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8393a24593424db663bc0996eb5bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8243f8880ed54cbd98cfd4286298b7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index_id', 'text', 'labels'],\n",
       "        num_rows: 701\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index_id', 'text', 'labels'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index_id', 'text', 'labels'],\n",
       "        num_rows: 204\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and fix your label conversion code:\n",
    "categories = [\"science/technology\", \"travel\", \"politics\", \"sports\", \"health\", \"entertainment\", \"geography\"]\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(categories)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "def convert_labels(example):\n",
    "    example[\"labels\"] = label2id[example[\"category\"]]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(convert_labels)\n",
    "ds = ds.remove_columns('category')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b4df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking data types to solve the error \"Unable to create tensors\"\n",
    "# [type(label) for label in ds['train'][\"labels\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1547a",
   "metadata": {},
   "source": [
    "### Using the XLM-RoBERTa model\n",
    "(https://huggingface.co/FacebookAI/xlm-roberta-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1abf918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e1698d60c543bd9aa9434305a49646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15f72922b124057b3453fd2bf309c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8438f6d8ae5b4c298b7f43ce72d1edf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832c55a0c8b742f7b5e60481702fb081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca401c39792b4543814961f0b754342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer for the XLM-RoBERTa model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "# Load XLM-RoBERTa foundation model \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-base\", num_labels=7,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)\n",
    "# Print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4935cb4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7ca34e5e054033b5797a51370f8778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3ed7cc0f1e4984b1c13953b9db243c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450fa4ba47bd4b61a5e71a041c5d0030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 701\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 99\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 204\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "def preprocessing(data):    \n",
    "    tokenized = tokenizer(data['text'], truncation=True, padding=True, max_length=512)\n",
    "    tokenized['labels']=data['labels']\n",
    "    return tokenized\n",
    "\n",
    "tokenized_data = ds.map(preprocessing, batched=True, remove_columns=['text','index_id'])\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75503f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\"eval_accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225be728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Load pretrained model and evaluate model after each epoch\n",
    "args = TrainingArguments(output_dir=\"./fm_telugu\", per_device_eval_batch_size=8)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Model Accuracy: 10.78%\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(f\"Foundation Model Accuracy: {results['eval_accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac384f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "XLM-RoBERTa model has an Accuracy of 10.78%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,961,422 || all params: 280,414,478 || trainable%: 1.056087410722067\n"
     ]
    }
   ],
   "source": [
    "# PEFT Model - based on the XLM-RoBERTa\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "## Config for PEFT\n",
    "# bnb_config  = BitsAndBytesConfig(load_in_8bit=True,\n",
    "#                                 bnb_8bit_use_double_quant=True)\n",
    "lora_config = LoraConfig(r=32,\n",
    "                         lora_alpha=64,\n",
    "                         lora_dropout=0.2,\n",
    "                         target_modules=[\"query\", \"key\", \"value\"],                                \n",
    "                         task_type=TaskType.SEQ_CLS,\n",
    "                         bias=\"none\")\n",
    "\n",
    "## Get Base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-base\",\n",
    "                                                                num_labels=7,\n",
    "                                                                id2label=id2label,\n",
    "                                                                label2id=label2id, \n",
    "#                                                                 quantization_config=bnb_config,                                                                \n",
    "                                                                device_map=\"auto\")\n",
    "## Prepare for k-bit training\n",
    "# model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "## Get peft model\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "## Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1760' max='1760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1760/1760 02:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.214340</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.189126</td>\n",
       "      <td>0.767677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>1.120553</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>1.381401</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>1.519963</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>1.298708</td>\n",
       "      <td>0.797980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>1.470326</td>\n",
       "      <td>0.797980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>1.357994</td>\n",
       "      <td>0.828283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>1.418454</td>\n",
       "      <td>0.808081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>1.405328</td>\n",
       "      <td>0.808081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /tmp/peft_telugu/checkpoint-176 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory /tmp/peft_telugu/checkpoint-352 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1760, training_loss=0.19673018022017044, metrics={'train_runtime': 141.3052, 'train_samples_per_second': 49.609, 'train_steps_per_second': 12.455, 'total_flos': 462751596165000.0, 'train_loss': 0.19673018022017044, 'epoch': 10.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collator is set to default\n",
    "# Compute metrics are the same\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/peft_telugu\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "\n",
    "# Train PEFT model \n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/peft_telugu/tokenizer_config.json',\n",
       " '/tmp/peft_telugu/special_tokens_map.json',\n",
       " '/tmp/peft_telugu/sentencepiece.bpe.model',\n",
       " '/tmp/peft_telugu/added_tokens.json',\n",
       " '/tmp/peft_telugu/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "peft_model.save_pretrained(\"/tmp/peft_telugu\")\n",
    "tokenizer.save_pretrained(\"/tmp/peft_telugu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Load the fine tuned model\n",
    "loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"/tmp/peft_telugu\")\n",
    "\n",
    "# Evaluate the peft model \n",
    "args = TrainingArguments(output_dir=\"./peft_telugu_eval\", per_device_eval_batch_size=8)\n",
    "trainer = Trainer(\n",
    "    model=loaded_peft_model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "peft_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundation Model Accuracy: 0.10784313725490197\n",
      "PEFT Model Accuracy:       0.8284313725490197\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(f\"Foundation Model Accuracy: {results.get('eval_accuracy', results.get('accuracy', 'N/A'))}\")\n",
    "print(f\"PEFT Model Accuracy:       {peft_results.get('eval_accuracy', peft_results.get('accuracy', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea5bf0",
   "metadata": {},
   "source": [
    "### Increase in performance on the dataset\n",
    "The foundation model once adapted, has a huge increase in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
